{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adapted from *Machine Learning with PyTorch and Scikit-Learn* by Raschka et al.  \n",
    "> Figures and selected functions used under CC BY-NC 4.0.  \n",
    "> Markdown content and examples modified for educational use in neuroscience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32fd7c",
   "metadata": {},
   "source": [
    "# Training Simple Machine Learning Algorithms for Classification\n",
    "**Chapter 2/3 — Adapted for SNEU20007U Computational Neuroscience**\n",
    "\n",
    "**This notebook contains interactive widgets.**  \n",
    "To run it live in your browser, click the badge below:\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/BergLab/CompNeuroCourse/HEAD?filepath=notebooks%2FML_joao%2Fch02_expanded.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This chapter covers two key linear classifiers:\n",
    "\n",
    "- The **Perceptron**, a spiking-neuron-inspired binary classifier\n",
    "- **Adaline**, which improves on it using gradient descent\n",
    "\n",
    "We’ll learn to apply these algorithms to classify **synthetic neuron waveforms**, and understand the role of:\n",
    "- Thresholds and activation functions\n",
    "- Weight updates\n",
    "- Loss functions and convergence\n",
    "- Feature standardization\n",
    "- Online learning (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neurons – A Brief Glimpse into the Early History of Machine Learning\n",
    "\n",
    "Before diving into perceptrons and modern learning algorithms, it's worth appreciating their historical roots in neuroscience and logic.\n",
    "\n",
    "<!-- <img src=\"./figures/02_01.png\" width=\"600\"> -->\n",
    "![Bio Neuron](figures/02_01.png)\n",
    "\n",
    "In 1943, **Warren McCulloch** and **Walter Pitts** published a seminal paper where they proposed a simple binary model of a neuron: the **McCulloch-Pitts (MCP) neuron**. This artificial neuron would integrate multiple binary inputs, apply a threshold function, and produce a binary output — a simplified version of the “all-or-none” spiking behavior of biological neurons.\n",
    "\n",
    "> The MCP model was essentially a **logic gate dressed as a neuron**, but it laid the mathematical foundation for neural networks.\n",
    "\n",
    "A few years later, **Frank Rosenblatt** introduced the **Perceptron** — the first trainable artificial neuron. Inspired by the MCP model, Rosenblatt added a learning rule that could adjust the weights of inputs based on classification errors. This made the perceptron capable of *learning* from labeled data — effectively initiating the field of supervised machine learning.\n",
    "\n",
    "The perceptron decision rule is simple:\n",
    "- Take a linear combination of inputs and weights.\n",
    "- Add a bias term.\n",
    "- Pass the result through a unit step function to decide if the output should be 1 (fires) or 0 (doesn’t fire).\n",
    "\n",
    "This is, in spirit, not far from how a real neuron sums postsynaptic potentials and generates an action potential once a certain threshold is crossed.\n",
    "\n",
    "**Why should we care?**  \n",
    "Because this is one of the first times a biologically inspired idea directly shaped the birth of machine learning. And while today's models are far more complex, their **conceptual ancestry** is rooted in this simple neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055838fe",
   "metadata": {},
   "source": [
    "# The Perceptron: A Spiking-Inspired Linear Classifier\n",
    "\n",
    "Inspired by the early work of McCulloch and Pitts, the perceptron is one of the simplest models of a neuron. It receives multiple inputs, computes a weighted sum, adds a bias, and emits an output — a **binary spike** — based on a threshold.\n",
    "\n",
    "**Mathematical formulation:**\n",
    "\n",
    "$$ z = \\mathbf{w}^T \\mathbf{x} + b $$  \n",
    "$$ \\text{output} = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\\\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\mathbf{x}$: the input feature vector (e.g., spike width, amplitude, symmetry)\n",
    "- $\\mathbf{w}$: the weight vector, learned from data\n",
    "- $b$: the bias term, which shifts the decision boundary\n",
    "- $z$: the net input, i.e., the total signal received by the neuron\n",
    "\n",
    "The perceptron decides to \"fire\" (output 1) or remain silent (output 0) based on whether this total input $z$ crosses the threshold (which is zero, after absorbing it into the bias).\n",
    "\n",
    "In a neuroscience context, you can think of:\n",
    "- Each **input** $x_j$ as the presence or strength of a synaptic input\n",
    "- Each **weight** $w_j$ as a synaptic strength\n",
    "- The **bias** as an intrinsic excitability offset\n",
    "- The **output** as the neuron's spiking decision\n",
    "\n",
    "This makes the perceptron a rough abstraction of how a neuron integrates synaptic input to make a binary decision.\n",
    "\n",
    "<!-- <img src=\"./figures/02_02.png\" width=\"600\"> -->\n",
    "![Bio Neuron](figures/02_02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc4ce5",
   "metadata": {},
   "source": [
    "# The Perceptron Learning Rule\n",
    "\n",
    "Weight updates are made **only when a prediction is wrong**, according to this simple rule:\n",
    "\n",
    "$$ \\Delta w_j = \\eta (y^{(i)} - \\hat{y}^{(i)}) x_j^{(i)} $$\n",
    "$$ \\Delta b = \\eta (y^{(i)} - \\hat{y}^{(i)}) $$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ is the learning rate\n",
    "- $x_j^{(i)}$ is the $j$-th feature of the $i$-th sample\n",
    "- $y^{(i)}$ is the true label\n",
    "- $\\hat{y}^{(i)}$ is the predicted label (either 0 or 1)\n",
    "\n",
    "<!-- <img src=\"./figures/02_04.png\" width=\"600\"> -->\n",
    "![Perceptron Learning Rule](figures/02_04.png)\n",
    "\n",
    "In plain language:  \n",
    "> If the perceptron makes a mistake, it **nudges the weights** in a direction that would have produced the correct output.\n",
    "\n",
    "This rule keeps adjusting the model to move the **decision boundary** — the line (or hyperplane) that separates one class from the other — in a way that improves classification over time.\n",
    "\n",
    "## Linear Separability\n",
    "\n",
    "The perceptron algorithm **converges only if the data is linearly separable**.\n",
    "\n",
    "What does that mean?\n",
    "\n",
    "A dataset is linearly separable if we can draw a **single straight line** (or plane in higher dimensions) that perfectly separates all examples of one class from all examples of another.\n",
    "\n",
    "- Imagine plotting two types of neurons in a 2D space using their spike width and amplitude.\n",
    "- If you can draw a line such that all pyramidal cells are on one side and all interneurons are on the other, the data is linearly separable.\n",
    "\n",
    "> If the classes are **not** linearly separable — for example, if they overlap or form concentric patterns — the perceptron will **never converge**, and it will keep oscillating weight updates forever.\n",
    "\n",
    "<!-- <img src=\"./figures/02_03.png\" width=\"600\"> -->\n",
    "![Linear Separability](figures/02_03.png)\n",
    "\n",
    "This is a major limitation of the basic perceptron model.\n",
    "\n",
    "Later algorithms (like **Adaline**, **logistic regression**, and **support vector machines**) address this limitation by using **soft boundaries**, **nonlinear transformations**, or **optimization-based loss functions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4bab8f",
   "metadata": {},
   "source": [
    "# Perceptron in Code\n",
    "We define a `Perceptron` class with `fit()` and `predict()` methods, just like scikit-learn estimators.\n",
    "\n",
    "We’ll train it on our **binary neuron classification** problem using synthetic spike waveform features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float64(0.)\n",
    "        \n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_ += update * xi\n",
    "                self.b_ += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = 0.5 * v1\n",
    "np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch02_expanded import load_synthetic_neuron\n",
    "\n",
    "# Use the function and adapt downstream code\n",
    "X, y, df = load_synthetic_neuron(as_frame=True)\n",
    "\n",
    "# Match old Iris-style pipeline\n",
    "y = np.where(df['neuron_type'] == 'Interneuron', 0, 1)\n",
    "X_plot = df[['spike_width', 'amplitude']].values \n",
    "# dataset features: spike_width, amplitude, upstroke_downstroke, symmetry_index\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_plot[y == 0, 0], X_plot[y == 0, 1],\n",
    "            color='red', marker='o', label='Interneuron')\n",
    "plt.scatter(X_plot[y == 1, 0], X_plot[y == 1, 1],\n",
    "            color='blue', marker='s', label='Pyramidal')\n",
    "\n",
    "plt.xlabel('Spike Width [ms]')\n",
    "plt.ylabel('Amplitude [µV]')\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 2 features for training AND plotting\n",
    "X_plot = df[['spike_width', 'amplitude']].values\n",
    "y = np.where(df['neuron_type'] == 'Interneuron', 0, 1)\n",
    "\n",
    "# Train on 2D data\n",
    "ppn = Perceptron(eta=0.1, n_iter=10) #FIXME what n_iter is required for the Percepton to converge?\n",
    "ppn.fit(X_plot, y)\n",
    "\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of updates')\n",
    "sns.despine()\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    lab = lab.reshape(xx1.shape)\n",
    "    # plt.figure(figsize=(5, 4))\n",
    "    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=f'Class {cl}', \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then plot\n",
    "plot_decision_regions(X_plot, y, classifier=ppn)\n",
    "plt.xlabel('Spike Width [ms]')\n",
    "plt.ylabel('Amplitude [µV]')\n",
    "plt.legend(loc='upper left')\n",
    "#plt.savefig('images/02_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline: Adaptive Linear Neuron\n",
    "\n",
    "While the perceptron was an important milestone in the early days of neural networks, it had a key limitation:  \n",
    "**it only updates its weights based on discrete classification outcomes (0 or 1)**. That’s where **Adaline** comes in — a slightly more sophisticated model introduced by Widrow and Hoff in 1960.\n",
    "\n",
    "Adaline — short for **ADAptive LInear NEuron** — differs from the perceptron in a subtle but powerful way:\n",
    "- Instead of using the *final predicted class* for weight updates (as the perceptron does),\n",
    "- Adaline uses the **raw continuous output of the neuron** (the net input, a.k.a. activation), *before* thresholding.\n",
    "\n",
    "This subtle change means the model minimizes a **differentiable loss function**, the **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$ L = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y^{(i)} - z^{(i)}\\right)^2 $$\n",
    "\n",
    "Where:\n",
    "- $y^{(i)}$ is the true class label (0 or 1) \n",
    "- $z^{(i)}$ is the continuous-valued net input (before applying the step function)\n",
    "- $n$ is the number of training examples\n",
    "\n",
    "<!-- <img src=\"./figures/02_09.png\" width=\"600\"> -->\n",
    "![Adaline](figures/02_09.png)\n",
    "\n",
    "## Why does this matter?\n",
    "\n",
    "Because **differentiable loss functions** are the key to modern machine learning.  \n",
    "This allows Adaline to use **gradient descent** to update its weights:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla L(\\mathbf{w}) $$\n",
    "\n",
    "Where $\\eta$ is the learning rate, and the gradient tells us how the error changes with respect to each weight.  \n",
    "\n",
    "<!-- <img src=\"./figures/02_10.png\" width=\"600\"> -->\n",
    "![Gradient Descent](figures/02_10.png)\n",
    "\n",
    "This makes Adaline a kind of **gateway drug** to deeper neural networks and algorithms like logistic regression or backpropagation.\n",
    "\n",
    "## Conceptual Summary\n",
    "\n",
    "- **Perceptron**: Makes weight updates based on *predicted class* errors (non-differentiable)\n",
    "- **Adaline**: Makes updates based on the **real-valued activation output** → differentiable MSE loss\n",
    "\n",
    "Both models ultimately produce binary predictions, but Adaline trains in a smoother, more principled way — which improves stability, especially when the data isn’t perfectly linearly separable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD:\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : Scalar\n",
    "      Bias unit after fitting.\n",
    "    losses_ : list\n",
    "      Mean squared eror loss function values in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1])\n",
    "        self.b_ = np.float64(0.)\n",
    "        self.losses_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            # Please note that the \"activation\" method has no effect\n",
    "            # in the code since it is simply an identity function. We\n",
    "            # could write `output = self.net_input(X)` directly instead.\n",
    "            # The purpose of the activation is more conceptual, i.e.,  \n",
    "            # in the case of logistic regression (as we will see later), \n",
    "            # we could change it to\n",
    "            # a sigmoid function to implement a logistic regression classifier.\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            \n",
    "            #for w_j in range(self.w_.shape[0]):\n",
    "            #    self.w_[w_j] += self.eta * (2.0 * (X[:, w_j]*errors)).mean()\n",
    "            \n",
    "            self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]\n",
    "            self.b_ += self.eta * 2.0 * errors.mean()\n",
    "            loss = (errors**2).mean()\n",
    "            self.losses_.append(loss)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the sy\n",
    "X, y, df = load_synthetic_neuron(as_frame=True)\n",
    "X_plot = df[['spike_width', 'amplitude']].values\n",
    "y = np.where(df['neuron_type'] == 'Interneuron', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dynamics\n",
    "\n",
    "Adaline’s loss surface is **convex**, which means:\n",
    "- Gradient descent can reliably find the global minimum\n",
    "- The algorithm will converge, as long as the learning rate isn’t too high\n",
    "\n",
    "<!-- <img src=\"./figures/02_12.png\" width=\"600\"> -->\n",
    "![Training Dynamics](figures/02_12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and visualize\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "ada1 = AdalineGD(n_iter=15, eta=0.1).fit(X_plot, y)\n",
    "ax[0].plot(range(1, len(ada1.losses_) + 1), np.log10(ada1.losses_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Mean squared error)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.1')\n",
    "\n",
    "ada2 = AdalineGD(n_iter=15, eta=0.0001).fit(X_plot, y)\n",
    "ax[1].plot(range(1, len(ada2.losses_) + 1), ada2.losses_, marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "plot_decision_regions(X_plot, y, classifier=ppn)      # Perceptron\n",
    "plt.title('Perceptron')\n",
    "plt.figure(figsize=(5, 4))\n",
    "plot_decision_regions(X_plot, y, classifier=ada1)     # Adaline (with eta=0.1)\n",
    "plt.title('Adaline (with eta=0.1)')\n",
    "plt.figure(figsize=(5, 4))\n",
    "plot_decision_regions(X_plot, y, classifier=ada2)     # Adaline (with eta=0.0001)\n",
    "plt.title('Adaline (with eta=0.0001)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatLogSlider, IntSlider\n",
    "\n",
    "# Interactive Adaline training plot for exploration\n",
    "def interactive_adaline(eta, epochs):\n",
    "    model = AdalineGD(eta=eta, n_iter=epochs).fit(X_plot, y)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.plot(range(1, len(model.losses_) + 1), model.losses_, marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title(f'Adaline: eta={eta}, epochs={epochs}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the widget\n",
    "interact(interactive_adaline,\n",
    "         eta=FloatLogSlider(base=10, value=0.01, min=-5, max=-1, step=0.1, description='Learning Rate'),\n",
    "         epochs=IntSlider(value=15, min=2, max=50, step=1, description='Epochs'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Gradient Descent Through Feature Scaling\n",
    "\n",
    "Gradient descent is a powerful optimization algorithm, but it's also quite sensitive to the scale of the input features. When features have very different scales — for example, `spike_width` in milliseconds vs. `amplitude` in microvolts — the optimizer may zigzag inefficiently, slowing convergence or even diverging entirely.\n",
    "\n",
    "To mitigate this, we apply a preprocessing step called **standardization**:\n",
    "\n",
    "$$ x'_j = \\frac{x_j - \\mu_j}{\\sigma_j} $$\n",
    "\n",
    "Here, each feature is centered around zero and scaled to have unit variance. This helps gradient descent **take uniform steps across all dimensions**, leading to faster and more stable convergence.\n",
    "\n",
    "<!-- <img src=\"./figures/02_13.png\" width=\"600\"> -->\n",
    "![Feature Scaling](figures/02_13.png)\n",
    "\n",
    "> In practice, scaling can make the difference between a model that learns in 10 epochs vs. one that never converges at all.\n",
    "\n",
    "We'll now apply standardization to our synthetic neuron dataset and observe how it improves Adaline's training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the sy\n",
    "X, y, df = load_synthetic_neuron(as_frame=True)\n",
    "X_plot = df[['spike_width', 'amplitude']].values\n",
    "y = np.where(df['neuron_type'] == 'Interneuron', 0, 1)\n",
    "\n",
    "# standardize features\n",
    "X_std = np.copy(X_plot)\n",
    "X_std[:, 0] = (X_plot[:, 0] - X_plot[:, 0].mean()) / X_plot[:, 0].std()\n",
    "X_std[:, 1] = (X_plot[:, 1] - X_plot[:, 1].mean()) / X_plot[:, 1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_gd = AdalineGD(n_iter=20, eta=0.5)\n",
    "ada_gd.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada_gd)\n",
    "plt.title('Adaline - Gradient descent')\n",
    "plt.xlabel('Sepal length [standardized]')\n",
    "plt.ylabel('Petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/02_14_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(range(1, len(ada_gd.losses_) + 1), ada_gd.losses_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean squared error')\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "#plt.savefig('images/02_14_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "\n",
    "While **batch gradient descent** updates the weights only after processing the entire training set, **Stochastic Gradient Descent (SGD)** takes a different, more dynamic approach.\n",
    "\n",
    "SGD updates the model parameters **after each individual training example**, which leads to:\n",
    "\n",
    "- **Faster convergence**, especially in large datasets\n",
    "- **More frequent updates**, allowing the model to respond quickly\n",
    "- **Online learning**, where the model can learn continuously from new data as it arrives\n",
    "\n",
    "## Why use SGD?\n",
    "\n",
    "In practice, re-evaluating the entire training dataset every time (as done in batch gradient descent) becomes computationally expensive — especially when working with large-scale data, like high-throughput brain recordings.\n",
    "\n",
    "SGD solves this by **approximating the gradient** using just one example at a time:\n",
    "\n",
    "$$ \\Delta w_j = \\eta \\, (y^{(i)} - z^{(i)}) \\, x_j^{(i)} $$  \n",
    "$$ \\Delta b = \\eta \\, (y^{(i)} - z^{(i)}) $$\n",
    "\n",
    "While this introduces some \"noise\" in the optimization (the loss function fluctuates more), it can help the algorithm escape shallow local minima — a desirable property when optimizing nonlinear models.\n",
    "\n",
    "## Practical Notes\n",
    "\n",
    "- **Shuffling** the training data between epochs is essential to avoid cycles.\n",
    "- **Adaptive learning rates** (which decrease over time) can help the model converge more smoothly.\n",
    "- **Mini-batch gradient descent** (training on small batches of examples) is often used as a middle ground.\n",
    "\n",
    "> In neuroscience terms, you can think of SGD as a synaptic update that happens right after each \"spike\" of training data — a much more biologically plausible mechanism than batch processing the entire brain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD:\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    shuffle : bool (default: True)\n",
    "      Shuffles training data every epoch if True to prevent cycles.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    b_ : Scalar\n",
    "        Bias unit after fitting.\n",
    "    losses_ : list\n",
    "      Mean squared error loss function value averaged over all\n",
    "      training examples in each epoch.\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.losses_ = []\n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            losses = []\n",
    "            for xi, target in zip(X, y):\n",
    "                losses.append(self._update_weights(xi, target))\n",
    "            avg_loss = np.mean(losses)\n",
    "            self.losses_.append(avg_loss)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        return self\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"Shuffle training data\"\"\"\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=m)\n",
    "        self.b_ = np.float64(0.)\n",
    "        self.w_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target - output)\n",
    "        self.w_ += self.eta * 2.0 * xi * (error)\n",
    "        self.b_ += self.eta * 2.0 * error\n",
    "        loss = error**2\n",
    "        return loss\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_) + self.b_\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_sgd = AdalineSGD(n_iter=15, eta=0.01, random_state=1)\n",
    "ada_sgd.fit(X_std, y)\n",
    "\n",
    "plot_decision_regions(X_std, y, classifier=ada_sgd)\n",
    "plt.title('Adaline - Stochastic gradient descent')\n",
    "plt.xlabel('Sepal length [standardized]')\n",
    "plt.ylabel('Petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_15_1.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ada_sgd.losses_) + 1), ada_sgd.losses_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average loss')\n",
    "\n",
    "plt.savefig('figures/02_15_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatLogSlider, IntSlider, Dropdown\n",
    "\n",
    "# Reload the sy\n",
    "X, y, df = load_synthetic_neuron(as_frame=True)\n",
    "X_plot = df[['spike_width', 'amplitude']].values\n",
    "y = np.where(df['neuron_type'] == 'Interneuron', 0, 1)\n",
    "\n",
    "# Standardize data\n",
    "X_std = (X_plot - X_plot.mean(axis=0)) / X_plot.std(axis=0)\n",
    "\n",
    "# Comparison widget for batch vs stochastic gradient descent\n",
    "def compare_gd_vs_sgd(eta, epochs, optimizer_type):\n",
    "    if optimizer_type == 'Batch GD':\n",
    "        model = AdalineGD(eta=eta, n_iter=epochs).fit(X_std, y)\n",
    "        label = 'Batch Gradient Descent'\n",
    "    else:\n",
    "        model = AdalineSGD(eta=eta, n_iter=epochs, random_state=1).fit(X_std, y)\n",
    "        label = 'Stochastic Gradient Descent'\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    plt.sca(axs[0])\n",
    "    plot_decision_regions(X_std, y, classifier=model)\n",
    "    axs[0].set_xlabel('Spike Width [standardized]')\n",
    "    axs[0].set_ylabel('Amplitude [standardized]')\n",
    "    axs[0].set_title(label)\n",
    "    axs[0].legend(loc='upper left')\n",
    "\n",
    "    axs[1].plot(range(1, len(model.losses_) + 1), model.losses_, marker='o')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_title('Loss Curve')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "interact(compare_gd_vs_sgd,\n",
    "         eta=FloatLogSlider(base=10, value=0.01, min=-5, max=1, step=0.1, description='Learning Rate'),\n",
    "         epochs=IntSlider(value=15, min=3, max=50, step=1, description='Epochs'),\n",
    "         optimizer_type=Dropdown(options=['Batch GD', 'Stochastic GD'], value='Batch GD', description='Optimizer'));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Gradient Descent Strategies\n",
    "\n",
    "Use the interactive widget above to:\n",
    "\n",
    "- Toggle between **Batch Gradient Descent** and **Stochastic Gradient Descent (SGD)**\n",
    "- Observe how the **decision boundary** evolves with each optimizer\n",
    "- Monitor the **loss curve** over epochs\n",
    "- Explore how different **learning rates** affect convergence and stability\n",
    "\n",
    "Try extreme values for the learning rate or change the number of epochs — and see how the optimizer behaves!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Build and Compare a Custom Neuron Classifier\n",
    "\n",
    "In this exercise, you will explore how simple linear classifiers behave when trained on different combinations of features from the synthetic neuron dataset.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Use the **synthetic neuron dataset** (2-class version).\n",
    "2. Choose **any two features** of your choice (e.g., spike width and symmetry index).\n",
    "3. Train both a:\n",
    "   - **Perceptron** model\n",
    "   - **Adaline** model\n",
    "4. For Adaline, try **two different learning rates** (e.g., `0.01` and `0.0001`) and visualize the **loss over epochs**.\n",
    "5. Plot the **decision boundaries** for both classifiers on a 2D scatter plot.\n",
    "6. Write a short reflection:\n",
    "   - Which model converged more quickly?\n",
    "   - Which produced a smoother decision boundary?\n",
    "   - How did learning rate affect the Adaline model’s performance?\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Understand how feature choice and learning rate affect model behavior.\n",
    "- Compare Perceptron vs. Adaline learning rules in practice.\n",
    "- Gain confidence interpreting training curves and decision surfaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Exercise: Build and Compare a Custom Neuron Classifier ===\n",
    "\n",
    "# 1. Import the synthetic 2-class dataset\n",
    "# from ch02_expanded import load_synthetic_neuron\n",
    "\n",
    "# X, y = load_synthetic_neuron(as_frame=False)\n",
    "# Pick two feature indices you want to use, e.g., 0 and 3\n",
    "\n",
    "# 2. Select two features only\n",
    "# X = X[:, [feature_index_1, feature_index_2]]\n",
    "\n",
    "# 3. Standardize the features\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_std = \n",
    "\n",
    "# 4. Train a Perceptron\n",
    "# from ch02_expanded import Perceptron\n",
    "# ppn = \n",
    "# ppn.\n",
    "\n",
    "# 5. Train two Adaline models with different learning rates\n",
    "# from ch02_expanded import AdalineGD\n",
    "# ada1 = AdalineGD(eta=..., n_iter=...).\n",
    "# ada2 = AdalineGD(eta=..., n_iter=...).\n",
    "\n",
    "# 6. Plot decision boundaries\n",
    "# Use the provided plot_decision_regions function from the notebook\n",
    "\n",
    "# 7. Plot training curves\n",
    "# plt.plot(ada1.losses_) and plt.plot(ada2.losses_)\n",
    "\n",
    "# 8. Write a short reflection based on the plots and results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
